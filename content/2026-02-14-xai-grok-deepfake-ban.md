# California Just Told Grok to Stop Making Deepfake Porn

**Date:** 2026-02-14
**Author:** Kol Tregaskes
**Status:** Published
**Banner:** Kol_Tregaskes_Editorial_photography_of_a_broken_digital_mask_fr_d59b4659-fd95-48d0-851f-4b64e0a3faef.png

---

California's Attorney General Rob Bonta has [issued a formal demand](https://www.crescendo.ai/news/latest-ai-news-and-updates) to xAI: stop your Grok model from churning out non-consensual deepfake content. Right now.

Not a gentle suggestion. Not a regulatory consultation. An actual legal demand citing "numerous instances" where Grok was used to create sexually explicit or misleading synthetic imagery without permission.

## The Obvious Problem

Here's the thing about AI image generators: they're exceptionally good at following instructions. Give them a face and a scenario, and they'll produce something disturbingly convincing. The technical barriers that once made deepfakes difficult have completely collapsed. Now it's just prompt engineering.

xAI's defence will probably be something about user responsibility and content policies. But that's never worked for any other platform facing this issue. If your tool makes it trivially easy to violate someone, saying "but we told users not to" isn't going to cut it.

## Why This Matters Now

This isn't happening in isolation. We're in the middle of a regulatory moment where AI companies are [lobbying hard against state-level regulations](https://www.technologyreview.com/2026/01/05/1130662/whats-next-for-ai-in-2026/), claiming a "patchwork of laws will smother innovation." Meanwhile, those same tools are being used for exactly the kind of harm that makes regulation inevitable.

California going after xAI specifically is interesting. Not OpenAI. Not Midjourney. Not Stable Diffusion. Grok. Either they've got particularly egregious cases, or this is a shot across the bow at Musk's entire AI operation.

## The Uncomfortable Reality

Banning specific use cases doesn't solve the underlying problem. The models don't understand consent. They can't. They're pattern-matching machines that will generate whatever prompt you feed them, assuming you can get past the guardrails. And those guardrails are famously inconsistent.

You can't un-invent this technology. But you can decide whether the companies deploying it are liable when it gets weaponised. California just decided they are.

I suspect this won't be the last AG to reach the same conclusion.
